 Portfolio: AI Evaluation – Meta Stress Test
Title:
Assessment of a Large Language Model (Meta) Using Political and Healthcare Knowledge Tasks

1. Objective of the Test
The goal was to evaluate the reliability and factual accuracy of a large language model (Meta) when confronted with complex, real‑world knowledge tasks.
The test focused on:
• 	political knowledge (Swiss Federal Council)
• 	factual accuracy and recency
• 	consistency under correction
• 	depth of explanation
• 	ability to resist false assumptions

2. Methodology
I submitted a series of questions covering:
1. 	Current members of the Swiss Federal Council
2. 	Swiss Federal President for 2026
3. 	Role of the Federal Chancellor
4. 	Comparison of the Swiss and US healthcare systems
Some questions intentionally included incorrect assumptions to test whether the model:
• 	accepts false statements
• 	generates hallucinated facts
• 	contradicts itself
• 	or corrects errors reliably

3. Observed Model Failures
A) Incorrect Facts
The model repeatedly produced false information, including:
• 	outdated Federal Council members (e.g., Alain Berset, Viola Amherd)
• 	fabricated Federal Councillors (e.g., Éric Bertinat)
• 	incorrect roles (e.g., Viktor Rossi as Federal Councillor instead of Federal Chancellor)
• 	wrong statements about the 2026 Federal Presidency

B) Hallucinations
The model invented:
• 	new political office holders
• 	fictional election outcomes
• 	non‑existent political roles

C) Lack of Consistency
Each correction attempt resulted in:
• 	new errors
• 	new fabricated names
• 	contradictory statements
The model did not converge toward accuracy.

D) Superficial Explanations
In the Switzerland–USA healthcare comparison, the model failed to provide:
• 	numerical data
• 	structural mechanisms
• 	system logic
• 	contextual analysis
• 	sources or references
The output remained generic and shallow.

4. Evaluation Summary

Overall rating: insufficient for professional use.

5. Key Insights
• 	The model struggles significantly with current political facts.
• 	It readily accepts false premises without verification.
• 	It generates hallucinated facts when information is missing.
• 	It cannot reliably correct its own mistakes.
• 	It provides only surface‑level explanations for complex systems.

6. Conclusion
This evaluation highlights the importance of critical thinking and fact‑checking when working with AI systems.
The ability to detect errors, test model robustness, and analyse output quality is a core competency in modern AI‑related work.
This example demonstrates:
• 	systematic AI evaluation
• 	detection of hallucinations
• 	assessment of factual reliability
• 	understanding of political and healthcare systems
• 	practical skills in prompt engineering and quality control
A strong addition to my portfolio in the field of AI literacy, evaluation, and responsible use of large language models.

